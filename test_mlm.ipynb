{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\r\n",
    "from tensorflow.python.data.ops.dataset_ops import AUTOTUNE\r\n",
    "from transformers import BertTokenizer, TFBertModel"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "feature_description = { # 定义Feature结构，告诉解码器每个Feature的类型是什么\r\n",
    "    'id': tf.io.FixedLenFeature([], tf.string),\r\n",
    "    'tag_id': tf.io.VarLenFeature(tf.int64),\r\n",
    "    'category_id': tf.io.FixedLenFeature([], tf.int64),\r\n",
    "    'title': tf.io.FixedLenFeature([], tf.string),\r\n",
    "    'asr_text': tf.io.FixedLenFeature([], tf.string),\r\n",
    "    'frame_feature': tf.io.VarLenFeature(tf.string)\r\n",
    "}\r\n",
    "\r\n",
    "\r\n",
    "def read_and_decode(example_string):\r\n",
    "    '''\r\n",
    "    从TFrecord格式文件中读取数据 train\r\n",
    "    '''\r\n",
    "    feature_dict = tf.io.parse_single_example(example_string, feature_description)\r\n",
    "    frame_feature = tf.sparse.to_dense(feature_dict['frame_feature']).numpy()\r\n",
    "    title = feature_dict['title'].numpy()\r\n",
    "    asr_text = feature_dict['asr_text'].numpy()\r\n",
    "    id = feature_dict['id'].numpy()\r\n",
    "    tag_id = tf.sparse.to_dense(feature_dict['tag_id']).numpy()\r\n",
    "    category_id = feature_dict['category_id'].numpy()\r\n",
    "\r\n",
    "\r\n",
    "    return id, tag_id, category_id, frame_feature, title, asr_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "filenames = 'data/pairwise/pairwise.tfrecords'\r\n",
    "dataset = tf.data.TFRecordDataset(filenames)\r\n",
    "datas = {}\r\n",
    "for i, data in enumerate(dataset):\r\n",
    "    id, tag_id, category_id, frame_feature, title, asr_text = read_and_decode(data)\r\n",
    "    datas[i] = [title.decode('utf-8'), asr_text.decode('utf-8')]\r\n",
    "    datas['title'] = title.decode('utf-8')\r\n",
    "    datas['asr_text'] = asr_text.decode('utf-8')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "datas[0].decode('utf-8')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'英雄联盟：8年未曾拿过五杀，电脑都看不下去，直接接管了！'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def mask_tokens(inputs, mlm_probability, tokenizer, special_tokens_mask):\r\n",
    "    \"\"\"\r\n",
    "    Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\r\n",
    "    \"\"\"\r\n",
    "    labels = np.copy(inputs)\r\n",
    "    # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\r\n",
    "    probability_matrix = np.random.random_sample(labels.shape)\r\n",
    "    special_tokens_mask = special_tokens_mask.astype(np.bool_)\r\n",
    "\r\n",
    "    probability_matrix[special_tokens_mask] = 0.0\r\n",
    "    masked_indices = probability_matrix > (1 - mlm_probability)\r\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\r\n",
    "\r\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\r\n",
    "    indices_replaced = (np.random.random_sample(labels.shape) < 0.8) & masked_indices\r\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\r\n",
    "\r\n",
    "    # 10% of the time, we replace masked input tokens with random word\r\n",
    "    indices_random = (np.random.random_sample(labels.shape) < 0.5) & masked_indices & ~indices_replaced\r\n",
    "    random_words = np.random.randint(low=0, high=len(tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64)\r\n",
    "    inputs[indices_random] = random_words\r\n",
    "\r\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\r\n",
    "    return inputs, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "title = datas[0].decode('utf-8')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "title"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'英雄联盟：8年未曾拿过五杀，电脑都看不下去，直接接管了！'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('data/chinese-roberta-wwm-ext')\r\n",
    "max_bert_length = 32\r\n",
    "mlm_probability = 0.15"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "encoded_inputs = tokenizer(title, max_length=max_bert_length, padding='max_length', truncation=True, return_special_tokens_mask=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "encoded_inputs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [101, 5739, 7413, 5468, 4673, 8038, 129, 2399, 3313, 3295, 2897, 6814, 758, 3324, 8024, 4510, 5554, 6963, 4692, 679, 678, 1343, 8024, 4684, 2970, 2970, 5052, 749, 8013, 102, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "example = tokenizer.pad(encoded_inputs, return_tensors=\"np\", pad_to_multiple_of=None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "example"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': array([ 101, 5739, 7413, 5468, 4673, 8038,  129, 2399, 3313, 3295, 2897,\n",
       "       6814,  758, 3324, 8024, 4510, 5554, 6963, 4692,  679,  678, 1343,\n",
       "       8024, 4684, 2970, 2970, 5052,  749, 8013,  102,    0,    0]), 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'special_tokens_mask': array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1]), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0])}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "special_tokens_mask = example.pop(\"special_tokens_mask\", None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "special_tokens_mask"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "example"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': array([ 101, 5739, 7413, 5468, 4673, 8038,  129, 2399, 3313, 3295, 2897,\n",
       "       6814,  758, 3324, 8024, 4510, 5554, 6963, 4692,  679,  678, 1343,\n",
       "       8024, 4684, 2970, 2970, 5052,  749, 8013,  102,    0,    0]), 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0])}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "example[\"input_ids\"], example[\"labels\"] = mask_tokens(\r\n",
    "    example[\"input_ids\"], mlm_probability, tokenizer, special_tokens_mask=special_tokens_mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "example"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': array([  101, 10068,  7413,   103,   103,   103,   129,  2399,  3313,\n",
       "        3295,  2897,  6814,   758,   103,   103,  4510,   103,  6963,\n",
       "        4692,   679,   678,  1343,  8024,   103,  2970,   103,  5052,\n",
       "         749,   103,   102,     0,     0]), 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0]), 'labels': array([-100, 5739, -100, 5468, 4673, 8038, -100, -100, -100, -100, -100,\n",
       "       -100, -100, 3324, -100, 4510, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100,  103, -100, -100,  103, -100, -100, -100])}"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "example[\"labels\"][example[\"labels\"] == tokenizer.pad_token_id] = -100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "example[\"labels\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-100, 5739, -100, 5468, 4673, 8038, -100, -100, -100, -100, -100,\n",
       "       -100, -100, 3324, -100, 4510, -100, -100, -100, -100, -100, -100,\n",
       "       -100, -100, -100,  103, -100, -100,  103, -100, -100, -100])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "example = {key: tf.convert_to_tensor(arr) for key, arr in example.items()}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "example"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([  101, 10068,  7413,   103,   103,   103,   129,  2399,  3313,\n",
       "         3295,  2897,  6814,   758,   103,   103,  4510,   103,  6963,\n",
       "         4692,   679,   678,  1343,  8024,   103,  2970,   103,  5052,\n",
       "          749,   103,   102,     0,     0])>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])>,\n",
       " 'attention_mask': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0])>,\n",
       " 'labels': <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([-100, 5739, -100, 5468, 4673, 8038, -100, -100, -100, -100, -100,\n",
       "        -100, -100, 3324, -100, 4510, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100,  103, -100, -100,  103, -100, -100, -100])>}"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "title = title.numpy().decode(encoding='utf-8')\r\n",
    "encoded_inputs = tokenizer(title, max_length=max_bert_length, padding='max_length', truncation=True, return_special_tokens_mask=True)\r\n",
    "example = tokenizer.pad(encoded_inputs, return_tensors=\"np\", pad_to_multiple_of=None)\r\n",
    "special_tokens_mask = example.pop(\"special_tokens_mask\", None)\r\n",
    "example[\"input_ids\"], example[\"labels\"] = mask_tokens(\r\n",
    "    example[\"input_ids\"], mlm_probability, tokenizer, special_tokens_mask=special_tokens_mask)\r\n",
    "if tokenizer.pad_token_id is not None:\r\n",
    "    example[\"labels\"][example[\"labels\"] == tokenizer.pad_token_id] = -100\r\n",
    "example = {key: tf.convert_to_tensor(arr) for key, arr in example.items()}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "bert = TFBertModel.from_pretrained('data/chinese-roberta-wwm-ext')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at data/chinese-roberta-wwm-ext.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "bert.config"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"data/chinese-roberta-wwm-ext\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.9.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('tf2': conda)"
  },
  "interpreter": {
   "hash": "0b763893c5a0ab4a9d687dc489be7391bc321afd58f24791d0d0ca036456de32"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}